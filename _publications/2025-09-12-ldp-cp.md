---
title: "Privacy-Preserving Conformal Prediction Under Local Differential Privacy"
collection: publications
category: conferences
permalink: /publication/2025-09-12-ldp-conformal-prediction
excerpt: "We propose two LDP conformal prediction frameworks that compute thresholds from privatized data without true labels—(i) users send features and a k-ary randomized-response label (no model access) and (ii) users privatize conformity via binary-search responses (requires model access, stricter privacy)—and prove finite-sample coverage with robust performance under strong randomization for sensitive applications."
# date: 2025-09-12
venue: "COPA 2025"
slidesurl: '/files/penso_mahpud_goldberger_sheffet_copa2025_slides.pdf'
paperurl: '/files/penso_mahpud_goldberger_sheffet_copa2025_paper.pdf'
bibtexurl: '/files/penso_mahpud_goldberger_sheffet_copa2025.bib'
citation: 'Penso, Coby; Mahpud, Bar; Goldberger, Jacob; Sheffet, Or. (2025). &quot;Privacy-Preserving Conformal Prediction Under Local Differential Privacy.&quot; <i>COPA 2025</i>.'
---
Conformal prediction (CP) provides sets of candidate classes with a guaranteed probability of containing the true class. However, it typically relies on a calibration set with clean labels. We address privacy-sensitive scenarios where the aggregator is untrusted and can only access a perturbed version of the true labels. We propose two complementary approaches under local differential privacy (LDP). In the first approach, users do not access the model but instead provide their input features and a perturbed label using a k-ary randomized response. In the second approach, which enforces stricter privacy constraints, users add noise to their conformity score by binary search response. This method requires access to the classification model but preserves both data and label privacy. Both approaches compute the conformal threshold directly from noisy data without accessing the true labels. We prove finite-sample coverage guarantees and demonstrate robust coverage even under severe randomization. This approach unifies strong local privacy with predictive uncertainty control, making it well-suited for sensitive applications such as medical imaging or large language model queries, regardless of whether users can (or are willing to) compute their own scores.